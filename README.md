ğŸš€ Job Role Prediction Project

ğŸ“– Overview

This project is part of my Infosys Internship and aims to predict suitable job roles for candidates based on their educational background, certifications, and skills using Machine Learning.
The work is divided into structured milestones, ensuring a systematic and professional approach from data preparation to final deployment.

ğŸ¯ Objectives

ğŸŸ¢ Predict job roles from resumes and candidate profiles

ğŸŸ¢ Build an end-to-end ML pipeline (data cleaning â†’ feature engineering â†’ model training â†’ deployment)

ğŸŸ¢ Demonstrate strong knowledge of EDA, feature engineering, ML models, and deployment tools

ğŸ† Milestones
âœ… Milestone 1 â€“ Data Cleaning & Exploratory Data Analysis

ğŸ““ Notebook: Infosys(M-1).ipynb

ğŸ”¹ Collected and integrated datasets (Education, Certifications, Skills, Job Roles)

ğŸ”¹ Cleaned and preprocessed the raw dataset

ğŸ”¹ Handled missing values and standardized text fields

ğŸ”¹ Performed Exploratory Data Analysis (EDA) with meaningful visualizations

ğŸ“Œ Outputs:

ğŸŸ¡ Cleaned dataset ready for ML

ğŸŸ¡ EDA report notebook

Perfect ğŸ‘ â€” hereâ€™s a **well-formatted and professional explanation** of your entire process, written in a way thatâ€™s ideal for uploading to **GitHub** (for your Milestone or preprocessing notebook).
You can paste this directly into your `README.md`, `Milestone_Report.md`, or as a documentation section in your project folder.
It clearly explains every stage step-by-step, including purpose, methods, and outcomes.



# ğŸ§  Milestone-2: Data Preprocessing and Visualization

 ğŸ” **Overview**

This milestone focuses on **cleaning, transforming, and visualizing** the *Edu2Job Dataset* to prepare it for machine learning model development.
The process ensures the data is consistent, well-structured, and free from issues such as missing values, outliers, or format inconsistencies.


 âš™ï¸ **Steps Followed**

 **1ï¸âƒ£ Load the Dataset**

* The dataset `Edu2Job_dataset_.csv` is loaded using **Pandas**.
* Initial inspection includes displaying:

  * Dataset shape
  * Column names
  * Sample data
* Verifies that the file is correctly located and accessible.

   python
df = pd.read_csv(file_path)
print("Shape:", df.shape)
print("Columns:", df.columns.tolist())

**2ï¸âƒ£ Handling Missing Values**

Missing data can reduce model accuracy if not managed properly.

* **Numeric Columns** â†’ filled with the **mean** of that column.
* **Categorical Columns** â†’ filled with the **most frequent value (mode)** or `"Unknown"` if mode doesnâ€™t exist.
* This approach keeps dataset size consistent and prevents errors during model training.

 python
num_cols = df.select_dtypes(include=[np.number]).columns
cat_cols = df.select_dtypes(exclude=[np.number]).columns


âœ… *All missing values handled safely â€” no warnings.*

**3ï¸âƒ£ Handling Outliers (IQR Method)**

Outliers can distort statistical analysis and model performance.
Used the **Interquartile Range (IQR)** method:

* Values below **Q1 âˆ’ 1.5 Ã— IQR** or above **Q3 + 1.5 Ã— IQR** are **capped** to boundary values.
* This preserves data shape but prevents extreme deviations.

âœ… *Outliers successfully capped.*


 **4ï¸âƒ£ Label Encoding for Categorical Data**

Machine-learning models require numeric input.

* Applied **LabelEncoder** to convert text data into numeric form.
* Stored encoders for decoding later if needed.

Example:

| Degree | Encoded |
| ------ | ------- |
| B.Tech | 0       |
| M.Tech | 1       |
| MBA    | 2       |

âœ… *All categorical features encoded numerically.*



 **5ï¸âƒ£ Feature Scaling (Standardization)**

To ensure all numeric features are on a similar scale:

* Used **StandardScaler** â†’ transforms features to have **mean = 0**, **std = 1**.
* Prevents large-scale features (like GPA or Experience) from dominating the model.

âœ… *Numeric features standardized successfully.*



 **6ï¸âƒ£ Feature Selection**

Separated data into:

* **X:** Input features (Degree, Major, GPA, Certifications, Skills, Industry, Experience Level)
* **y:** Target variable (`Job Role`)

This prepares the dataset for supervised learning.

âœ… *Features and target variable successfully defined.*


 **7ï¸âƒ£ Train-Test Split**

To evaluate model performance objectively:

* Split the dataset â†’ **70 % training** and **30 % testing**.
* Used `stratify=y` to keep job-role category proportions consistent.
* Random seed fixed (`random_state = 42`) for reproducibility.

âœ… *Train/Test split completed successfully.*

 **8ï¸âƒ£ Save Preprocessed Data**

Two CSV files were exported for transparency:

* **Before preprocessing:** `Edu2Job_Before_Preprocessing.csv`
* **After preprocessing:**  `Edu2Job_After_Preprocessing.csv`

âœ… *Files saved for documentation and reproducibility.*



 **9ï¸âƒ£ Visualizations**

To explore feature relationships, several plots were generated using **Seaborn** and **Matplotlib**:

| Plot                                        | Description                                      |
| ------------------------------------------- | ------------------------------------------------ |
| **Job Role Distribution**                   | Frequency of each job role                       |
| **Degree vs Job Role**                      | How degrees relate to job placement              |
| **GPA vs Job Role (Boxplot)**               | GPA variation across roles                       |
| **Certifications vs Job Role (Strip Plot)** | Certification influence                          |
| **Industry Distribution**                   | Count of samples in each industry                |
| **Experience Level vs Job Role**            | Relationship between experience and job type     |
| **Correlation Heatmap**                     | Strength of relationships among numeric features |

âœ… *Visualization completed successfully â€” helped reveal important insights.*



## ğŸ“Š **Final Results**

| Step                     | Status |
| ------------------------ | ------ |
| Dataset Loaded           | âœ…      |
| Missing Values Handled   | âœ…      |
| Outliers Capped          | âœ…      |
| Label Encoding           | âœ…      |
| Feature Scaling          | âœ…      |
| Train-Test Split         | âœ…      |
| Visualizations           | âœ…      |
| Preprocessed Files Saved | âœ…      |

**Dataset Summary**

* Rows: 1000â€ƒColumns: 8
* Target: *Job Role*
* Minimum accuracy potential: High (clean & standardized data)

## ğŸ§¾ **Conclusion**

This milestone establishes a **clean, well-structured dataset** ready for model building.
By handling missing data, normalizing scales, encoding categories, and visualizing relationships, we created a solid foundation for predictive modeling â€” specifically, predicting job roles based on educational background and skillset.

Next step â†’ proceed to **model training and evaluation** (Milestone-3).

Edu2Job_Project/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ Edu2Job_dataset_.csv
â”‚   â”œâ”€â”€ Edu2Job_Before_Preprocessing.csv
â”‚   â””â”€â”€ Edu2Job_After_Preprocessing.csv
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ Edu2Job_Preprocessing.ipynb
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ Edu2Job_Preprocessing.py
â”‚
â”œâ”€â”€ visuals/
â”‚   â””â”€â”€ output_plots/
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt




